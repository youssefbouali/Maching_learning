import numpy as np

def descente_de_gradient_stochastique(f_gradient, x_init, donnees, taux_apprentissage=0.1, max_iterations=50, tolerance=1e-6):
    """
    Algorithme de descente de gradient stochastique (SGD) avec crit√®re de convergence.
    :param f_gradient: Fonction qui calcule le gradient pour un √©chantillon donn√©.
    :param x_init: Valeur initiale de x.
    :param donnees: Donn√©es d'entra√Ænement (y_i).
    :param taux_apprentissage: Taux d'apprentissage (ùúÇ).
    :param max_iterations: Nombre maximal d'it√©rations (N).
    :param tolerance: Seuil de convergence pour arr√™ter les it√©rations.
    :return: La valeur finale de x.
    """
    x = x_init  # Initialisation de x avec la valeur donn√©e
    
    for t in range(max_iterations):
        # Sauvegarder la valeur pr√©c√©dente de x
        x_precedent = x

        # S√©lectionner un √©chantillon al√©atoire
        i = np.random.randint(0, len(donnees))
        echantillon = donnees[i]
        
        # Calculer le gradient pour l'√©chantillon
        grad = f_gradient(x, echantillon)
        
        # Mettre √† jour x
        x = x - taux_apprentissage * grad

        # Afficher l'√©volution (optionnel)
        print(f"It√©ration {t+1} : x = {x:.4f} pour x_old={i} y={echantillon}")

        # V√©rifier le crit√®re de convergence
        if abs(x - x_precedent) < tolerance:
            print(f"Convergence atteinte √† l'it√©ration {t+1}")
            break
    
    return x

# Fonction gradient pour une √©quation quadratique
def gradient_quadratique(x, echantillon):
    return 2 * (x - echantillon)

# Donn√©es d'entra√Ænement (y_i)
donnees = np.array([1.0, 2.0, 3.0, 4.0, 5.0])

# Initialisation des param√®tres
x_initial = 0.0  # Valeur initiale de x
taux_apprentissage = 0.1  # Taux d'apprentissage
max_iterations = 1000  # Nombre maximal d'it√©rations
tolerance = 1e-6  # Seuil de convergence

# Ex√©cuter l'algorithme
x_final = descente_de_gradient_stochastique(gradient_quadratique, x_initial, donnees, taux_apprentissage, max_iterations, tolerance)

print(f"\nValeur finale de x : {x_final:.4f}")

#data= [1.0, 2.0, 3.0, 4.0, 5.0]
#x= 0.0
#Gradient= 2(x‚àíy3 )=2(0.0‚àí3.0)= ‚àí6.0
#Œ∑ =0.1
#x.new =x.old ‚àíŒ∑√óGradient=0.0‚àí0.1√ó(‚àí6.0)=0.6

#La valeur optimale de x est la valeur qui minimise l'√©cart entre x et toutes les valeurs des donn√©es y. En d'autres termes, apr√®s plusieurs √©tapes de mise √† jour al√©atoire, on arrive √† une valeur de x qui est proche du **moyenne des valeurs** =3